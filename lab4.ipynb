{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Assignment Four: Multi-Layer Perceptron \n",
    "In this lab, you will compare the performance of multi-layer perceptrons programmed  via your own various implementations. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (one per team) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. This lab project is slightly different from other reports in that you will be asked to complete more specific items.\n",
    "\n",
    "Dataset Selection\n",
    "\n",
    "For this assignment, you will be using a specific dataset chosen by the instructor.  This is US Census data available on Kaggle, and also downloadable from the following link: https://www.dropbox.com/s/bf7i7qjftk7cmzq/acs2017_census_tract_data.csv?dl=0Links to an external site.\n",
    "\n",
    "The Kaggle description appears here: https://www.kaggle.com/muonneutrino/us-census-demographic-data/dataLinks to an external site. \n",
    "\n",
    "The classification task you will be performing is to predict, for each tract, what the child poverty rate will be. You will need to convert this from regression to four levels of classification by quantizing the variable of interest. \n",
    "\n",
    "Grading Rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Split, and Balance (1.5 points total)\n",
    "[.5 points] \n",
    "- (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric. \n",
    "- (2) Remove any observations that having missing data. \n",
    "- (3) Encode any string data as integers for now. \n",
    "- (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data into memory and save it to a pandas data frame\n",
    "df = pd.read_csv('./acs2017_census_tract_data.csv')\n",
    "\n",
    "# Remove any observations that have missing data\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode any string data as integers\n",
    "df = df.apply(lambda x: pd.factorize(x)[0] if x.dtype == 'object' else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two requirements will need to be completed together as they might depend on one another:\n",
    "- [.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. \n",
    "\n",
    "Should balancing of the dataset be done for both the training and testing set? Explain.\n",
    "\n",
    "No, it should only be done for training. Generally, we don't want to touch the testing set because adjusting the distribution of classes in the testing set could introduce bias and affect the evaluation of the model's generalization performance.\n",
    "\n",
    "\n",
    "- [.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is no need to split the data multiple times for this lab.\n",
    "Note: You will need to one hot encode the target, but do not one hot encode the categorical data until instructed to do so in the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TractId', 'State', 'County', 'TotalPop', 'Men', 'Women', 'Hispanic',\n",
      "       'White', 'Black', 'Native', 'Asian', 'Pacific', 'VotingAgeCitizen',\n",
      "       'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n",
      "       'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n",
      "       'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',\n",
      "       'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n",
      "       'SelfEmployed', 'FamilyWork', 'Unemployment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_ranges = [0, 50000, 100000, float('inf')]\n",
    "income_labels = ['<50000', '50000-100000', '>100000']\n",
    "\n",
    "df['IncomeEncoded'] = pd.cut(df['Income'], bins=income_ranges, labels=income_labels)\n",
    "\n",
    "df_encoded = pd.get_dummies(df['IncomeEncoded'])\n",
    "df = pd.concat([df, df_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.dropna()\n",
    "# y = y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iincome' is our target column\n",
    "X = df.drop('IncomeEncoded', axis=1)  # Features\n",
    "y = df['IncomeEncoded']  # Target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balance the training set by quantizing the \"ChildPoverty\" variable into four classes\n",
    "X_train['ChildPoverty'] = pd.qcut(X_train['ChildPoverty'], q=4, labels=False)\n",
    "\n",
    "\n",
    "# # One-hot encode the target variable\n",
    "# y_train_encoded = pd.get_dummies(y_train)\n",
    "# y_test_encoded = pd.get_dummies(y_test)\n",
    "\n",
    "\n",
    "# # Normalize the continuous numeric feature data\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TractId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IncomeEncoded</th>\n",
       "      <th>&lt;50000</th>\n",
       "      <th>50000-100000</th>\n",
       "      <th>&gt;100000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001020100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1845</td>\n",
       "      <td>899</td>\n",
       "      <td>946</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>881</td>\n",
       "      <td>74.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>50000-100000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001020200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2172</td>\n",
       "      <td>1167</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>852</td>\n",
       "      <td>75.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>&lt;50000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001020300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3385</td>\n",
       "      <td>1533</td>\n",
       "      <td>1852</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1482</td>\n",
       "      <td>73.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>&lt;50000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001020400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4267</td>\n",
       "      <td>2001</td>\n",
       "      <td>2266</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1849</td>\n",
       "      <td>75.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>50000-100000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001020500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9965</td>\n",
       "      <td>5054</td>\n",
       "      <td>4911</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4787</td>\n",
       "      <td>71.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>50000-100000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TractId  State  County  TotalPop   Men  Women  Hispanic  White  Black  \\\n",
       "0  1001020100      0       0      1845   899    946       2.4   86.3    5.2   \n",
       "1  1001020200      0       0      2172  1167   1005       1.1   41.6   54.5   \n",
       "2  1001020300      0       0      3385  1533   1852       8.0   61.4   26.5   \n",
       "3  1001020400      0       0      4267  2001   2266       9.6   80.3    7.1   \n",
       "4  1001020500      0       0      9965  5054   4911       0.9   77.5   16.4   \n",
       "\n",
       "   Native  ...  Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  \\\n",
       "0     0.0  ...       881         74.2        21.2           4.5         0.0   \n",
       "1     0.0  ...       852         75.9        15.0           9.0         0.0   \n",
       "2     0.6  ...      1482         73.3        21.1           4.8         0.7   \n",
       "3     0.5  ...      1849         75.8        19.7           4.5         0.0   \n",
       "4     0.0  ...      4787         71.4        24.1           4.5         0.0   \n",
       "\n",
       "   Unemployment  IncomeEncoded  <50000  50000-100000  >100000  \n",
       "0           4.6   50000-100000   False          True    False  \n",
       "1           3.4         <50000    True         False    False  \n",
       "2           4.7         <50000    True         False    False  \n",
       "3           6.1   50000-100000   False          True    False  \n",
       "4           2.3   50000-100000   False          True    False  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAIdCAYAAAAgdEmvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABUKElEQVR4nO3dfVxUdf7//+eIgogygshVklophagZliK1XoPmVVlfMwxlczWzNFTWXd3arP2k5WVtbuZaiaUutplWaxGYZctHMKUlRa1s07wIxBQGQQPE8/ujD/NrBOxo6EF43G+3ud2a837NOa8zTM2zc/Eem2EYhgAAAHBBjaxuAAAA4GpAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCLkFSUpJsNpvz0bRpUwUGBqpv376aN2+e8vPzq7xmzpw5stlsF7Wd06dPa86cOfrkk08u6nXVbatdu3YaOnToRa3nl6xdu1bPP/98tWM2m01z5syp1e3Vto8++kjdu3eXl5eXbDabNm7cWG3dwYMHZbPZtHDhwivb4FUsPj5ezZs3t7qNan3yySey2WwX/e8V0NjqBoCr2cqVK3XjjTeqvLxc+fn5Sk9P13PPPaeFCxdq3bp1GjBggLP2d7/7nQYNGnRR6z99+rSeeuopSVKfPn1Mv+5StnUp1q5dq5ycHCUkJFQZy8jIUJs2bS57D5fKMAyNGjVKHTt21LvvvisvLy+FhoZa3RaAOozQBPwK4eHh6t69u/P5Pffco2nTpun222/XyJEjtX//fgUEBEiS2rRpc9lDxOnTp9WsWbMrsq1f0rNnT0u3/0u+//57nTx5Unfffbf69+9vdTsArgKcngNq2bXXXqtFixbp1KlTWr58uXN5dafMtmzZoj59+qhVq1by9PTUtddeq3vuuUenT5/WwYMH1bp1a0nSU0895TwVGB8f77K+zz//XPfee698fHx0/fXX17itShs2bFCXLl3UtGlTXXfddfrrX//qMl556vHgwYMuy88/pdGnTx9t2rRJ3333ncupykrVnZ7LycnRiBEj5OPjo6ZNm+rmm2/WqlWrqt3OP/7xD/3pT39ScHCwvL29NWDAAH311Vc1v/E/k56erv79+6tFixZq1qyZevXqpU2bNjnH58yZ4wyVf/jDH2Sz2dSuXTtT665U+T59/PHHevjhh+Xn56dWrVpp5MiR+v7776vUr127VpGRkWrevLmaN2+um2++Wa+++qpLzWuvvaauXbuqadOm8vX11d133619+/a51FSe9vryyy8VExMjLy8vBQUF6dlnn5UkZWZm6vbbb5eXl5c6duxY5f2VzP0dJKmoqEiJiYlq37693N3ddc011yghIUElJSUX9V5dyObNm9W/f395e3urWbNmioqK0kcffeQc37hxo2w2m8uySsuWLZPNZtOuXbucy3bu3Knhw4fL19dXTZs2Vbdu3fTmm2/+Yh/ffvutRo8ereDgYHl4eCggIED9+/dXdnZ2rewn6gdCE3AZ3HnnnXJzc9Onn35aY83Bgwc1ZMgQubu767XXXlNKSoqeffZZeXl5qaysTEFBQUpJSZEkjR8/XhkZGcrIyNATTzzhsp6RI0fqhhtu0D//+U+9/PLLF+wrOztbCQkJmjZtmjZs2KBevXrpscceu6RrdV566SVFRUUpMDDQ2VtGRkaN9V999ZV69eqlPXv26K9//avefvtthYWFKT4+XvPnz69SP3v2bH333Xd65ZVX9Pe//1379+/XsGHDVFFRccG+tm7dqn79+snhcOjVV1/VP/7xD7Vo0ULDhg3TunXrJP10+vLtt9+WJE2ZMkUZGRnasGHDRb8Hletq0qSJ1q5dq/nz5+uTTz7RAw884FLz5z//WWPGjFFwcLCSkpK0YcMGjRs3Tt99952zZt68eRo/frw6deqkt99+Wy+88IJ27dqlyMhI7d+/32V95eXlGjlypIYMGaJ33nlHgwcP1qxZszR79myNGzdODz74oDZs2KDQ0FDFx8crKyvL+Vqzf4fTp0+rd+/eWrVqlaZOnaoPPvhAf/jDH5SUlKThw4fLMIxLer9+bvXq1YqOjpa3t7dWrVqlN998U76+voqJiXGGpKFDh8rf318rV66s8vqkpCTdcsst6tKliyTp448/VlRUlAoLC/Xyyy/rnXfe0c0336z77rtPSUlJF+zlzjvvVFZWlubPn6+0tDQtW7ZM3bp1U2Fh4a/eT9QjBoCLtnLlSkOSsWPHjhprAgICjJtuusn5/MknnzR+/q/cW2+9ZUgysrOza1zH8ePHDUnGk08+WWWscn1//vOfaxz7ubZt2xo2m63K9gYOHGh4e3sbJSUlLvt24MABl7qPP/7YkGR8/PHHzmVDhgwx2rZtW23v5/c9evRow8PDwzh06JBL3eDBg41mzZoZhYWFLtu58847XerefPNNQ5KRkZFR7fYq9ezZ0/D39zdOnTrlXHb27FkjPDzcaNOmjXHu3DnDMAzjwIEDhiRjwYIFF1xfTbWV79PkyZNdaufPn29IMnJzcw3DMIxvv/3WcHNzM8aMGVPj+gsKCgxPT88q+3zo0CHDw8PDiI2NdS4bN26cIclYv369c1l5ebnRunVrQ5Lx+eefO5efOHHCcHNzM6ZPn+5cZvbvMG/ePKNRo0ZVPuOVn9v333+/xv2p7NPLy6vG8ZKSEsPX19cYNmyYy/KKigqja9euxm233eZcNn36dMPT09PZm2EYxt69ew1JxosvvuhcduONNxrdunUzysvLXdY5dOhQIygoyKioqDAMo+pn+YcffjAkGc8///wF9wngSBNwmRi/8H/iN998s9zd3TVx4kStWrVK33777SVt55577jFd26lTJ3Xt2tVlWWxsrIqKivT5559f0vbN2rJli/r376+QkBCX5fHx8Tp9+nSVo1TDhw93eV55NOHnR2fOV1JSou3bt+vee+91uXPLzc1NcXFxOnLkiOlTfGb9Up9paWmqqKjQI488UuM6MjIydObMGeep10ohISHq169flVNTNptNd955p/N548aNdcMNNygoKEjdunVzLvf19ZW/v7/Le2b27/Cvf/1L4eHhuvnmm3X27FnnIyYmplbuPNu2bZtOnjypcePGuaz/3LlzGjRokHbs2OE8Dfjggw/qzJkzziOF0k83YXh4eCg2NlaS9M033+jLL7/UmDFjJMllnXfeeadyc3Nr/Nv7+vrq+uuv14IFC7R48WL95z//0blz537V/qF+IjQBl0FJSYlOnDih4ODgGmuuv/56bd68Wf7+/nrkkUd0/fXX6/rrr9cLL7xwUdsKCgoyXRsYGFjjshMnTlzUdi/WiRMnqu218j06f/utWrVyee7h4SFJOnPmTI3bKCgokGEYF7WdX+uX+jx+/LgkXfDC/Mqeaur7/J6bNWumpk2buixzd3eXr69vlde7u7vrxx9/dNmWmffn2LFj2rVrl5o0aeLyaNGihQzD0A8//FDj/phx7NgxSdK9995bZRvPPfecDMPQyZMnJf0U9m+99VbnKbqKigqtXr1aI0aMcO5z5foSExOrrG/y5MmSVGPPlddMxcTEaP78+brlllvUunVrTZ06VadOnfpV+4n6hbvngMtg06ZNqqio+MVpAu644w7dcccdqqio0M6dO/Xiiy8qISFBAQEBGj16tKltXczcT3l5eTUuq/zyr/wyLi0tdan7tV+SrVq1Um5ubpXllRdN+/n5/ar1S5KPj48aNWp02bdzMSov5j9y5EiVozuVKt/7mvquzZ7N/h38/Pzk6emp1157rdr1/NqeKl//4osv1ninZeWdp5L029/+VpMnT9a+ffv07bffKjc3V7/97W+rrG/WrFkaOXJkteu70JQSbdu2dV6Y//XXX+vNN9/UnDlzVFZW9ovXCqLh4EgTUMsOHTqkxMRE2e12PfTQQ6Ze4+bmph49euhvf/ubJDlPlZk5unIx9uzZoy+++MJl2dq1a9WiRQvdcsstkuS8i+zndyRJ0rvvvltlfR4eHqZ769+/v7Zs2VLlzrLXX39dzZo1q5UpCry8vNSjRw+9/fbbLn2dO3dOq1evVps2bdSxY8dfvZ2LER0dLTc3Ny1btqzGmsjISHl6emr16tUuy48cOeI8nVZbzP4dhg4dqv/+979q1aqVunfvXuVxsXcbni8qKkotW7bU3r17q11/9+7d5e7u7qy///771bRpUyUlJSkpKUnXXHONoqOjneOhoaHq0KGDvvjiixrX16JFC1O9dezYUY8//rg6d+582U9b4+rCkSbgV8jJyXFeN5Gfn69///vfWrlypdzc3LRhwwbnUYbqvPzyy9qyZYuGDBmia6+9Vj/++KPz/+orJ8Vs0aKF2rZtq3feeUf9+/eXr6+v/Pz8LvkLKzg4WMOHD9ecOXMUFBSk1atXKy0tTc8995yaNWsmSbr11lsVGhqqxMREnT17Vj4+PtqwYYPS09OrrK9z5856++23tWzZMkVERKhRo0Yu81b93JNPPql//etf6tu3r/785z/L19dXa9as0aZNmzR//nzZ7fZL2qfzzZs3TwMHDlTfvn2VmJgod3d3vfTSS8rJydE//vGPi56V/ddq166dZs+erb/85S86c+aM7r//ftntdu3du1c//PCDnnrqKbVs2VJPPPGEZs+erbFjx+r+++/XiRMn9NRTT6lp06Z68skna60fs3+HhIQErV+/Xr/5zW80bdo0denSRefOndOhQ4eUmpqqGTNmqEePHhfcVkVFhd56660qy728vDR48GC9+OKLGjdunE6ePKl7771X/v7+On78uL744gsdP37cJWi2bNlSd999t5KSklRYWKjExEQ1auT6//3Lly/X4MGDFRMTo/j4eF1zzTU6efKk9u3bp88//1z//Oc/q+1z165devTRR/X//t//U4cOHeTu7q4tW7Zo165d+uMf/3ixbzHqM2uvQweuTpV3TlU+3N3dDX9/f6N3797G3Llzjfz8/CqvOf+OtoyMDOPuu+822rZta3h4eBitWrUyevfubbz77rsur9u8ebPRrVs3w8PDw5BkjBs3zmV9x48f/8VtGcZPd88NGTLEeOutt4xOnToZ7u7uRrt27YzFixdXef3XX39tREdHG97e3kbr1q2NKVOmGJs2bapy99zJkyeNe++912jZsqVhs9lctqlq7vrbvXu3MWzYMMNutxvu7u5G165djZUrV7rUVN7Z9M9//tNleeUdbOfXV+ff//630a9fP8PLy8vw9PQ0evbsabz33nvVru/X3j13/t1l1d1laBiG8frrrxu33nqr0bRpU6N58+ZGt27dquzLK6+8YnTp0sVwd3c37Ha7MWLECGPPnj0uNTXdlda7d2+jU6dOVZZX/t1/zszfwTAMo7i42Hj88ceN0NBQZ0+dO3c2pk2bZuTl5VX3Vrn0+fN/R37++Pkdl1u3bjWGDBli+Pr6Gk2aNDGuueYaY8iQIVX+/oZhGKmpqc51fP3119Vu94svvjBGjRpl+Pv7G02aNDECAwONfv36GS+//LKz5vy/0bFjx4z4+HjjxhtvNLy8vIzmzZsbXbp0MZYsWWKcPXv2gvuJhsVmGLUw2QYAAEA9xzVNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwAQmt6xF586d0/fff68WLVpc8Qn0AADApTEMQ6dOnVJwcHCVSVN/jtBUi77//vsaf1sKAADUbYcPH77gj2sTmmpR5e8aHT58WN7e3hZ3AwAAzCgqKlJISMgv/j4hoakWVZ6S8/b2JjQBAHCV+aVLa7gQHAAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEywNDQtW7ZMXbp0cd5tFhkZqQ8++MA5Hh8fL5vN5vLo2bOnyzpKS0s1ZcoU+fn5ycvLS8OHD9eRI0dcagoKChQXFye73S673a64uDgVFha61Bw6dEjDhg2Tl5eX/Pz8NHXqVJWVlV22fQcAAFcXS0NTmzZt9Oyzz2rnzp3auXOn+vXrpxEjRmjPnj3OmkGDBik3N9f5eP/9913WkZCQoA0bNig5OVnp6ekqLi7W0KFDVVFR4ayJjY1Vdna2UlJSlJKSouzsbMXFxTnHKyoqNGTIEJWUlCg9PV3Jyclav369ZsyYcfnfBAAAcHUw6hgfHx/jlVdeMQzDMMaNG2eMGDGixtrCwkKjSZMmRnJysnPZ0aNHjUaNGhkpKSmGYRjG3r17DUlGZmamsyYjI8OQZHz55ZeGYRjG+++/bzRq1Mg4evSos+Yf//iH4eHhYTgcDtO9OxwOQ9JFvQYAAFjL7Pd3nbmmqaKiQsnJySopKVFkZKRz+SeffCJ/f3917NhREyZMUH5+vnMsKytL5eXlio6Odi4LDg5WeHi4tm3bJknKyMiQ3W5Xjx49nDU9e/aU3W53qQkPD1dwcLCzJiYmRqWlpcrKyqqx59LSUhUVFbk8AABA/WR5aNq9e7eaN28uDw8PTZo0SRs2bFBYWJgkafDgwVqzZo22bNmiRYsWaceOHerXr59KS0slSXl5eXJ3d5ePj4/LOgMCApSXl+es8ff3r7Jdf39/l5qAgACXcR8fH7m7uztrqjNv3jzndVJ2u53fnQMAoB6z/GdUQkNDlZ2drcLCQq1fv17jxo3T1q1bFRYWpvvuu89ZFx4eru7du6tt27batGmTRo4cWeM6DcNwmQq9umnRL6XmfLNmzdL06dOdzyt/uwYAANQ/lh9pcnd31w033KDu3btr3rx56tq1q1544YVqa4OCgtS2bVvt379fkhQYGKiysjIVFBS41OXn5zuPHAUGBurYsWNV1nX8+HGXmvOPKBUUFKi8vLzKEaif8/DwcN75x+/NAQBQv1kems5nGIbz9Nv5Tpw4ocOHDysoKEiSFBERoSZNmigtLc1Zk5ubq5ycHPXq1UuSFBkZKYfDoc8++8xZs337djkcDpeanJwc5ebmOmtSU1Pl4eGhiIiIWt9HAABw9bEZhmFYtfHZs2dr8ODBCgkJ0alTp5ScnKxnn31WKSkpioyM1Jw5c3TPPfcoKChIBw8e1OzZs3Xo0CHt27dPLVq0kCQ9/PDD+te//qWkpCT5+voqMTFRJ06cUFZWltzc3CT9dG3U999/r+XLl0uSJk6cqLZt2+q9996T9NNF6DfffLMCAgK0YMECnTx5UvHx8brrrrv04osvmt6foqIi2e12ORwOjjoBAHCVMPv9bek1TceOHVNcXJxyc3Nlt9vVpUsXpaSkaODAgTpz5ox2796t119/XYWFhQoKClLfvn21bt06Z2CSpCVLlqhx48YaNWqUzpw5o/79+yspKckZmCRpzZo1mjp1qvMuu+HDh2vp0qXOcTc3N23atEmTJ09WVFSUPD09FRsbq4ULF165N+MKavfHTVa3UG8cfHaI1S0AAK4QS4801TdXy5EmQlPtITQBwNXP7Pd3nbumCQAAoC4iNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAExpb3QAAtPvjJqtbqDcOPjvE6haAesvSI03Lli1Tly5d5O3tLW9vb0VGRuqDDz5wjhuGoTlz5ig4OFienp7q06eP9uzZ47KO0tJSTZkyRX5+fvLy8tLw4cN15MgRl5qCggLFxcXJbrfLbrcrLi5OhYWFLjWHDh3SsGHD5OXlJT8/P02dOlVlZWWXbd8BAMDVxdLQ1KZNGz377LPauXOndu7cqX79+mnEiBHOYDR//nwtXrxYS5cu1Y4dOxQYGKiBAwfq1KlTznUkJCRow4YNSk5OVnp6uoqLizV06FBVVFQ4a2JjY5Wdna2UlBSlpKQoOztbcXFxzvGKigoNGTJEJSUlSk9PV3JystavX68ZM2ZcuTcDAADUaTbDMAyrm/g5X19fLViwQA8++KCCg4OVkJCgP/zhD5J+OqoUEBCg5557Tg899JAcDodat26tN954Q/fdd58k6fvvv1dISIjef/99xcTEaN++fQoLC1NmZqZ69OghScrMzFRkZKS+/PJLhYaG6oMPPtDQoUN1+PBhBQcHS5KSk5MVHx+v/Px8eXt7m+q9qKhIdrtdDofD9GuswKmQ2sOpkNrBZ7L28JkELp7Z7+86cyF4RUWFkpOTVVJSosjISB04cEB5eXmKjo521nh4eKh3797atm2bJCkrK0vl5eUuNcHBwQoPD3fWZGRkyG63OwOTJPXs2VN2u92lJjw83BmYJCkmJkalpaXKysqqsefS0lIVFRW5PAAAQP1keWjavXu3mjdvLg8PD02aNEkbNmxQWFiY8vLyJEkBAQEu9QEBAc6xvLw8ubu7y8fH54I1/v7+Vbbr7+/vUnP+dnx8fOTu7u6sqc68efOc10nZ7XaFhIRc5N4DAICrheWhKTQ0VNnZ2crMzNTDDz+scePGae/evc5xm83mUm8YRpVl5zu/prr6S6k536xZs+RwOJyPw4cPX7AvAABw9bI8NLm7u+uGG25Q9+7dNW/ePHXt2lUvvPCCAgMDJanKkZ78/HznUaHAwECVlZWpoKDggjXHjh2rst3jx4+71Jy/nYKCApWXl1c5AvVzHh4ezjv/Kh8AAKB+sjw0nc8wDJWWlqp9+/YKDAxUWlqac6ysrExbt25Vr169JEkRERFq0qSJS01ubq5ycnKcNZGRkXI4HPrss8+cNdu3b5fD4XCpycnJUW5urrMmNTVVHh4eioiIuKz7CwAArg6WTm45e/ZsDR48WCEhITp16pSSk5P1ySefKCUlRTabTQkJCZo7d646dOigDh06aO7cuWrWrJliY2MlSXa7XePHj9eMGTPUqlUr+fr6KjExUZ07d9aAAQMkSTfddJMGDRqkCRMmaPny5ZKkiRMnaujQoQoNDZUkRUdHKywsTHFxcVqwYIFOnjypxMRETZgwgaNHAABAksWh6dixY4qLi1Nubq7sdru6dOmilJQUDRw4UJI0c+ZMnTlzRpMnT1ZBQYF69Oih1NRUtWjRwrmOJUuWqHHjxho1apTOnDmj/v37KykpSW5ubs6aNWvWaOrUqc677IYPH66lS5c6x93c3LRp0yZNnjxZUVFR8vT0VGxsrBYuXHiF3gkAAFDX1bl5mq5mzNPU8DAnTu3gM1l7+EwCF++qm6cJAACgLiM0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmWBqa5s2bp1tvvVUtWrSQv7+/7rrrLn311VcuNfHx8bLZbC6Pnj17utSUlpZqypQp8vPzk5eXl4YPH64jR4641BQUFCguLk52u112u11xcXEqLCx0qTl06JCGDRsmLy8v+fn5aerUqSorK7ss+w4AAK4uloamrVu36pFHHlFmZqbS0tJ09uxZRUdHq6SkxKVu0KBBys3NdT7ef/99l/GEhARt2LBBycnJSk9PV3FxsYYOHaqKigpnTWxsrLKzs5WSkqKUlBRlZ2crLi7OOV5RUaEhQ4aopKRE6enpSk5O1vr16zVjxozL+yYAAICrQmMrN56SkuLyfOXKlfL391dWVpZ+85vfOJd7eHgoMDCw2nU4HA69+uqreuONNzRgwABJ0urVqxUSEqLNmzcrJiZG+/btU0pKijIzM9WjRw9J0ooVKxQZGamvvvpKoaGhSk1N1d69e3X48GEFBwdLkhYtWqT4+Hg988wz8vb2vhxvAQAAuErUqWuaHA6HJMnX19dl+SeffCJ/f3917NhREyZMUH5+vnMsKytL5eXlio6Odi4LDg5WeHi4tm3bJknKyMiQ3W53BiZJ6tmzp+x2u0tNeHi4MzBJUkxMjEpLS5WVlVVtv6WlpSoqKnJ5AACA+qnOhCbDMDR9+nTdfvvtCg8Pdy4fPHiw1qxZoy1btmjRokXasWOH+vXrp9LSUklSXl6e3N3d5ePj47K+gIAA5eXlOWv8/f2rbNPf39+lJiAgwGXcx8dH7u7uzprzzZs3z3mNlN1uV0hIyKW/AQAAoE6z9PTczz366KPatWuX0tPTXZbfd999zn8ODw9X9+7d1bZtW23atEkjR46scX2GYchmszmf//yff03Nz82aNUvTp093Pi8qKiI4AQBQT9WJI01TpkzRu+++q48//lht2rS5YG1QUJDatm2r/fv3S5ICAwNVVlamgoICl7r8/HznkaPAwEAdO3asyrqOHz/uUnP+EaWCggKVl5dXOQJVycPDQ97e3i4PAABQP1kamgzD0KOPPqq3335bW7ZsUfv27X/xNSdOnNDhw4cVFBQkSYqIiFCTJk2UlpbmrMnNzVVOTo569eolSYqMjJTD4dBnn33mrNm+fbscDodLTU5OjnJzc501qamp8vDwUERERK3sLwAAuHpZenrukUce0dq1a/XOO++oRYsWziM9drtdnp6eKi4u1pw5c3TPPfcoKChIBw8e1OzZs+Xn56e7777bWTt+/HjNmDFDrVq1kq+vrxITE9W5c2fn3XQ33XSTBg0apAkTJmj58uWSpIkTJ2ro0KEKDQ2VJEVHRyssLExxcXFasGCBTp48qcTERE2YMIEjSAAAwNojTcuWLZPD4VCfPn0UFBTkfKxbt06S5Obmpt27d2vEiBHq2LGjxo0bp44dOyojI0MtWrRwrmfJkiW66667NGrUKEVFRalZs2Z677335Obm5qxZs2aNOnfurOjoaEVHR6tLly564403nONubm7atGmTmjZtqqioKI0aNUp33XWXFi5ceOXeEAAAUGfZDMMwrG6ivigqKpLdbpfD4ajTR6fa/XGT1S3UGwefHWJ1C/UCn8naw2cSuHhmv7/rxIXgAAAAdR2hCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwwdLQNG/ePN16661q0aKF/P39ddddd+mrr75yqTEMQ3PmzFFwcLA8PT3Vp08f7dmzx6WmtLRUU6ZMkZ+fn7y8vDR8+HAdOXLEpaagoEBxcXGy2+2y2+2Ki4tTYWGhS82hQ4c0bNgweXl5yc/PT1OnTlVZWdll2XcAAHB1sTQ0bd26VY888ogyMzOVlpams2fPKjo6WiUlJc6a+fPna/HixVq6dKl27NihwMBADRw4UKdOnXLWJCQkaMOGDUpOTlZ6erqKi4s1dOhQVVRUOGtiY2OVnZ2tlJQUpaSkKDs7W3Fxcc7xiooKDRkyRCUlJUpPT1dycrLWr1+vGTNmXJk3AwAA1Gk2wzAMq5uodPz4cfn7+2vr1q36zW9+I8MwFBwcrISEBP3hD3+Q9NNRpYCAAD333HN66KGH5HA41Lp1a73xxhu67777JEnff/+9QkJC9P777ysmJkb79u1TWFiYMjMz1aNHD0lSZmamIiMj9eWXXyo0NFQffPCBhg4dqsOHDys4OFiSlJycrPj4eOXn58vb2/sX+y8qKpLdbpfD4TBVb5V2f9xkdQv1xsFnh1jdQr3AZ7L28JkELp7Z7+86dU2Tw+GQJPn6+kqSDhw4oLy8PEVHRztrPDw81Lt3b23btk2SlJWVpfLycpea4OBghYeHO2syMjJkt9udgUmSevbsKbvd7lITHh7uDEySFBMTo9LSUmVlZVXbb2lpqYqKilweAACgfqozockwDE2fPl233367wsPDJUl5eXmSpICAAJfagIAA51heXp7c3d3l4+NzwRp/f/8q2/T393epOX87Pj4+cnd3d9acb968ec5rpOx2u0JCQi52twEAwFWizoSmRx99VLt27dI//vGPKmM2m83luWEYVZad7/ya6uovpebnZs2aJYfD4XwcPnz4gj0BAICr1yWFpuuuu04nTpyosrywsFDXXXfdRa9vypQpevfdd/Xxxx+rTZs2zuWBgYGSVOVIT35+vvOoUGBgoMrKylRQUHDBmmPHjlXZ7vHjx11qzt9OQUGBysvLqxyBquTh4SFvb2+XBwAAqJ8uKTQdPHjQ5c60SqWlpTp69Kjp9RiGoUcffVRvv/22tmzZovbt27uMt2/fXoGBgUpLS3MuKysr09atW9WrVy9JUkREhJo0aeJSk5ubq5ycHGdNZGSkHA6HPvvsM2fN9u3b5XA4XGpycnKUm5vrrElNTZWHh4ciIiJM7xMAAKifGl9M8bvvvuv85w8//FB2u935vKKiQh999JHatWtnen2PPPKI1q5dq3feeUctWrRwHumx2+3y9PSUzWZTQkKC5s6dqw4dOqhDhw6aO3eumjVrptjYWGft+PHjNWPGDLVq1Uq+vr5KTExU586dNWDAAEnSTTfdpEGDBmnChAlavny5JGnixIkaOnSoQkNDJUnR0dEKCwtTXFycFixYoJMnTyoxMVETJkzgCBIAALi40HTXXXdJ+unan3HjxrmMNWnSRO3atdOiRYtMr2/ZsmWSpD59+rgsX7lypeLj4yVJM2fO1JkzZzR58mQVFBSoR48eSk1NVYsWLZz1S5YsUePGjTVq1CidOXNG/fv3V1JSktzc3Jw1a9as0dSpU5132Q0fPlxLly51jru5uWnTpk2aPHmyoqKi5OnpqdjYWC1cuND0/gAAgPrrkuZpat++vXbs2CE/P7/L0dNVi3maGh7mxKkdfCZrD59J4OKZ/f6+qCNNlQ4cOHDJjQEAAFyNLik0SdJHH32kjz76SPn5+Tp37pzL2GuvvfarGwMAAKhLLik0PfXUU3r66afVvXt3BQUF/eKcSQAAAFe7SwpNL7/8spKSklx+8BYAAKA+u6R5msrKypzzGwEAADQElxSafve732nt2rW13QsAAECddUmn53788Uf9/e9/1+bNm9WlSxc1adLEZXzx4sW10hwAAEBdcUmhadeuXbr55pslSTk5OS5jXBQOAADqo0sKTR9//HFt9wEAAFCnXdI1TQAAAA3NJR1p6tu37wVPw23ZsuWSGwIAAKiLLik0VV7PVKm8vFzZ2dnKycmp8kO+AAAA9cElhaYlS5ZUu3zOnDkqLi7+VQ0BAADURbV6TdMDDzzA784BAIB6qVZDU0ZGhpo2bVqbqwQAAKgTLun03MiRI12eG4ah3Nxc7dy5U0888UStNAYAAFCXXFJostvtLs8bNWqk0NBQPf3004qOjq6VxgAAAOqSSwpNK1eurO0+AAAA6rRLCk2VsrKytG/fPtlsNoWFhalbt2611RcAAECdckmhKT8/X6NHj9Ynn3yili1byjAMORwO9e3bV8nJyWrdunVt9wkAAGCpS7p7bsqUKSoqKtKePXt08uRJFRQUKCcnR0VFRZo6dWpt9wgAAGC5SzrSlJKSos2bN+umm25yLgsLC9Pf/vY3LgQHAAD10iUdaTp37pyaNGlSZXmTJk107ty5X90UAABAXXNJoalfv3567LHH9P333zuXHT16VNOmTVP//v1rrTkAAIC64pJC09KlS3Xq1Cm1a9dO119/vW644Qa1b99ep06d0osvvljbPQIAAFjukq5pCgkJ0eeff660tDR9+eWXMgxDYWFhGjBgQG33BwAAUCdc1JGmLVu2KCwsTEVFRZKkgQMHasqUKZo6dapuvfVWderUSf/+978vS6MAAABWuqjQ9Pzzz2vChAny9vauMma32/XQQw9p8eLFtdYcAABAXXFRoemLL77QoEGDahyPjo5WVlbWr24KAACgrrmo0HTs2LFqpxqo1LhxYx0/fvxXNwUAAFDXXFRouuaaa7R79+4ax3ft2qWgoKBf3RQAAEBdc1Gh6c4779Sf//xn/fjjj1XGzpw5oyeffFJDhw6tteYAAADqiouacuDxxx/X22+/rY4dO+rRRx9VaGiobDab9u3bp7/97W+qqKjQn/70p8vVKwAAgGUuKjQFBARo27ZtevjhhzVr1iwZhiFJstlsiomJ0UsvvaSAgIDL0igAAICVLnpyy7Zt2+r9999XQUGBvvnmGxmGoQ4dOsjHx+dy9AcAAFAnXNKM4JLk4+OjW2+9tTZ7AQAAqLMu6bfnAAAAGhpCEwAAgAmEJgAAABMITQAAACZYGpo+/fRTDRs2TMHBwbLZbNq4caPLeHx8vGw2m8ujZ8+eLjWlpaWaMmWK/Pz85OXlpeHDh+vIkSMuNQUFBYqLi5PdbpfdbldcXJwKCwtdag4dOqRhw4bJy8tLfn5+mjp1qsrKyi7HbgMAgKuQpaGppKREXbt21dKlS2usGTRokHJzc52P999/32U8ISFBGzZsUHJystLT01VcXKyhQ4eqoqLCWRMbG6vs7GylpKQoJSVF2dnZiouLc45XVFRoyJAhKikpUXp6upKTk7V+/XrNmDGj9ncaAABclS55yoHaMHjwYA0ePPiCNR4eHgoMDKx2zOFw6NVXX9Ubb7yhAQMGSJJWr16tkJAQbd68WTExMdq3b59SUlKUmZmpHj16SJJWrFihyMhIffXVVwoNDVVqaqr27t2rw4cPKzg4WJK0aNEixcfH65lnnpG3t3ct7jUAALga1flrmj755BP5+/urY8eOmjBhgvLz851jWVlZKi8vV3R0tHNZcHCwwsPDtW3bNklSRkaG7Ha7MzBJUs+ePWW3211qwsPDnYFJkmJiYlRaWqqsrKwaeystLVVRUZHLAwAA1E91OjQNHjxYa9as0ZYtW7Ro0SLt2LFD/fr1U2lpqSQpLy9P7u7uVWYjDwgIUF5enrPG39+/yrr9/f1das7/+RcfHx+5u7s7a6ozb94853VSdrtdISEhv2p/AQBA3WXp6blfct999zn/OTw8XN27d1fbtm21adMmjRw5ssbXGYYhm83mfP7zf/41NeebNWuWpk+f7nxeVFREcAIAoJ6q00eazhcUFKS2bdtq//79kqTAwECVlZWpoKDApS4/P9955CgwMFDHjh2rsq7jx4+71Jx/RKmgoEDl5eUX/AFiDw8PeXt7uzwAAED9dFWFphMnTujw4cMKCgqSJEVERKhJkyZKS0tz1uTm5ionJ0e9evWSJEVGRsrhcOizzz5z1mzfvl0Oh8OlJicnR7m5uc6a1NRUeXh4KCIi4krsGgAAqOMsPT1XXFysb775xvn8wIEDys7Olq+vr3x9fTVnzhzdc889CgoK0sGDBzV79mz5+fnp7rvvliTZ7XaNHz9eM2bMUKtWreTr66vExER17tzZeTfdTTfdpEGDBmnChAlavny5JGnixIkaOnSoQkNDJUnR0dEKCwtTXFycFixYoJMnTyoxMVETJkzg6BEAAJBkcWjauXOn+vbt63xeeX3QuHHjtGzZMu3evVuvv/66CgsLFRQUpL59+2rdunVq0aKF8zVLlixR48aNNWrUKJ05c0b9+/dXUlKS3NzcnDVr1qzR1KlTnXfZDR8+3GVuKDc3N23atEmTJ09WVFSUPD09FRsbq4ULF17utwAAAFwlbIZhGFY3UV8UFRXJbrfL4XDU6SNU7f64yeoW6o2Dzw6xuoV6gc9k7eEzCVw8s9/fV9U1TQAAAFYhNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJlgamj799FMNGzZMwcHBstls2rhxo8u4YRiaM2eOgoOD5enpqT59+mjPnj0uNaWlpZoyZYr8/Pzk5eWl4cOH68iRIy41BQUFiouLk91ul91uV1xcnAoLC11qDh06pGHDhsnLy0t+fn6aOnWqysrKLsduAwCAq5CloamkpERdu3bV0qVLqx2fP3++Fi9erKVLl2rHjh0KDAzUwIEDderUKWdNQkKCNmzYoOTkZKWnp6u4uFhDhw5VRUWFsyY2NlbZ2dlKSUlRSkqKsrOzFRcX5xyvqKjQkCFDVFJSovT0dCUnJ2v9+vWaMWPG5dt5AABwVWls5cYHDx6swYMHVztmGIaef/55/elPf9LIkSMlSatWrVJAQIDWrl2rhx56SA6HQ6+++qreeOMNDRgwQJK0evVqhYSEaPPmzYqJidG+ffuUkpKizMxM9ejRQ5K0YsUKRUZG6quvvlJoaKhSU1O1d+9eHT58WMHBwZKkRYsWKT4+Xs8884y8vb2vwLsBAADqsjp7TdOBAweUl5en6Oho5zIPDw/17t1b27ZtkyRlZWWpvLzcpSY4OFjh4eHOmoyMDNntdmdgkqSePXvKbre71ISHhzsDkyTFxMSotLRUWVlZNfZYWlqqoqIilwcAAKif6mxoysvLkyQFBAS4LA8ICHCO5eXlyd3dXT4+Phes8ff3r7J+f39/l5rzt+Pj4yN3d3dnTXXmzZvnvE7KbrcrJCTkIvcSAABcLepsaKpks9lcnhuGUWXZ+c6vqa7+UmrON2vWLDkcDufj8OHDF+wLAABcvepsaAoMDJSkKkd68vPznUeFAgMDVVZWpoKCggvWHDt2rMr6jx8/7lJz/nYKCgpUXl5e5QjUz3l4eMjb29vlAQAA6qc6G5rat2+vwMBApaWlOZeVlZVp69at6tWrlyQpIiJCTZo0canJzc1VTk6OsyYyMlIOh0OfffaZs2b79u1yOBwuNTk5OcrNzXXWpKamysPDQxEREZd1PwEAwNXB0rvniouL9c033zifHzhwQNnZ2fL19dW1116rhIQEzZ07Vx06dFCHDh00d+5cNWvWTLGxsZIku92u8ePHa8aMGWrVqpV8fX2VmJiozp07O++mu+mmmzRo0CBNmDBBy5cvlyRNnDhRQ4cOVWhoqCQpOjpaYWFhiouL04IFC3Ty5EklJiZqwoQJHD0CAACSLA5NO3fuVN++fZ3Pp0+fLkkaN26ckpKSNHPmTJ05c0aTJ09WQUGBevToodTUVLVo0cL5miVLlqhx48YaNWqUzpw5o/79+yspKUlubm7OmjVr1mjq1KnOu+yGDx/uMjeUm5ubNm3apMmTJysqKkqenp6KjY3VwoULL/dbAAAArhI2wzAMq5uoL4qKimS32+VwOOr0Eap2f9xkdQv1xsFnh1jdQr3AZ7L28JkELp7Z7+86e00TAABAXUJoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMaGx1AwAA1EXt/rjJ6hbqhYPPDrG6hVrDkSYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMCEOh2a5syZI5vN5vIIDAx0jhuGoTlz5ig4OFienp7q06eP9uzZ47KO0tJSTZkyRX5+fvLy8tLw4cN15MgRl5qCggLFxcXJbrfLbrcrLi5OhYWFV2IXAQDAVaJOhyZJ6tSpk3Jzc52P3bt3O8fmz5+vxYsXa+nSpdqxY4cCAwM1cOBAnTp1ylmTkJCgDRs2KDk5Wenp6SouLtbQoUNVUVHhrImNjVV2drZSUlKUkpKi7OxsxcXFXdH9BAAAdVtjqxv4JY0bN3Y5ulTJMAw9//zz+tOf/qSRI0dKklatWqWAgACtXbtWDz30kBwOh1599VW98cYbGjBggCRp9erVCgkJ0ebNmxUTE6N9+/YpJSVFmZmZ6tGjhyRpxYoVioyM1FdffaXQ0NArt7MAAKDOqvNHmvbv36/g4GC1b99eo0eP1rfffitJOnDggPLy8hQdHe2s9fDwUO/evbVt2zZJUlZWlsrLy11qgoODFR4e7qzJyMiQ3W53BiZJ6tmzp+x2u7OmJqWlpSoqKnJ5AACA+qlOh6YePXro9ddf14cffqgVK1YoLy9PvXr10okTJ5SXlydJCggIcHlNQECAcywvL0/u7u7y8fG5YI2/v3+Vbfv7+ztrajJv3jzndVB2u10hISGXvK8AAKBuq9OhafDgwbrnnnvUuXNnDRgwQJs2bZL002m4SjabzeU1hmFUWXa+82uqqzeznlmzZsnhcDgfhw8f/sV9AgAAV6c6HZrO5+Xlpc6dO2v//v3O65zOPxqUn5/vPPoUGBiosrIyFRQUXLDm2LFjVbZ1/PjxKkexzufh4SFvb2+XBwAAqJ+uqtBUWlqqffv2KSgoSO3bt1dgYKDS0tKc42VlZdq6dat69eolSYqIiFCTJk1canJzc5WTk+OsiYyMlMPh0Geffeas2b59uxwOh7MGAACgTt89l5iYqGHDhunaa69Vfn6+/ud//kdFRUUaN26cbDabEhISNHfuXHXo0EEdOnTQ3Llz1axZM8XGxkqS7Ha7xo8frxkzZqhVq1by9fVVYmKi83SfJN10000aNGiQJkyYoOXLl0uSJk6cqKFDh3LnHAAAcKrToenIkSO6//779cMPP6h169bq2bOnMjMz1bZtW0nSzJkzdebMGU2ePFkFBQXq0aOHUlNT1aJFC+c6lixZosaNG2vUqFE6c+aM+vfvr6SkJLm5uTlr1qxZo6lTpzrvshs+fLiWLl16ZXcWAADUaTbDMAyrm6gvioqKZLfb5XA46vT1Te3+uMnqFuqNg88OsbqFeoHPZO3hM1l7+FzWjqvhM2n2+/uquqYJAADAKoQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQtN5XnrpJbVv315NmzZVRESE/v3vf1vdEgAAqAMITT+zbt06JSQk6E9/+pP+85//6I477tDgwYN16NAhq1sDAAAWIzT9zOLFizV+/Hj97ne/00033aTnn39eISEhWrZsmdWtAQAAizW2uoG6oqysTFlZWfrjH//osjw6Olrbtm2r9jWlpaUqLS11Pnc4HJKkoqKiy9doLThXetrqFuqNuv63vlrwmaw9fCZrD5/L2nE1fCYrezQM44J1hKb/88MPP6iiokIBAQEuywMCApSXl1fta+bNm6ennnqqyvKQkJDL0iPqHvvzVncAuOIzibrmavpMnjp1Sna7vcZxQtN5bDaby3PDMKosqzRr1ixNnz7d+fzcuXM6efKkWrVqVeNr8MuKiooUEhKiw4cPy9vb2+p2AEl8LlH38JmsPYZh6NSpUwoODr5gHaHp//j5+cnNza3KUaX8/PwqR58qeXh4yMPDw2VZy5YtL1eLDY63tzf/IUCdw+cSdQ2fydpxoSNMlbgQ/P+4u7srIiJCaWlpLsvT0tLUq1cvi7oCAAB1BUeafmb69OmKi4tT9+7dFRkZqb///e86dOiQJk2aZHVrAADAYoSmn7nvvvt04sQJPf3008rNzVV4eLjef/99tW3b1urWGhQPDw89+eSTVU59Albic4m6hs/klWczfun+OgAAAHBNEwAAgBmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACk1uiTti/f7+2bdumvLw82Ww2BQQEqFevXurQoYPVrQEAIInQBIs5HA6NHTtW7733nux2u/z9/WUYho4fP66ioiINGzZMr7/+Oj9GiStux44dev7556sN89OmTVP37t2tbhENzJEjR7Rs2bJqP5OTJk1SSEiI1S3We8wIDkuNHTtW2dnZWrFihXr06OEytn37dk2cOFE333yzVq1aZVGHaIg2btyoUaNGqX///oqJiVFAQIAMw1B+fr5SU1P10Ucf6c0339SIESOsbhUNRHp6ugYPHqyQkBBFR0e7fCbT0tJ0+PBhffDBB4qKirK61XqN0ARLtWzZUh9++GGVwFQpMzNTgwYNUmFh4ZVtDA1aeHi4HnjgAf3xj3+sdvy5557T66+/rj179lzhztBQ3Xrrrbr99tu1ZMmSasenTZum9PR07dix4wp31rBwITgsZ7PZLmkMuFy++eYbjRw5ssbxu+66S//973+vYEdo6HJycjRp0qQaxx966CHl5ORcwY4aJkITLDVs2DBNmDBBO3furDK2c+dOTZo0ScOHD7egMzRk119/vTZu3Fjj+DvvvKPrrrvuyjWEBi8oKEjbtm2rcTwjI0NBQUFXsKOGiQvBYakXX3xR999/v2677Ta1bNlS/v7+stlsOnbsmBwOh2JiYvTXv/7V6jbRwDz99NMaPXq0tm7d6rx+xGazKS8vT2lpaUpNTVVycrLVbaIBSUxM1KRJk5SVlaWBAwdW+Uy+8sorev75561us97jmibUCfv27VNmZqby8vIkSYGBgYqMjNSNN95ocWdoqDIyMvTCCy8oIyOjyufyscceU2RkpMUdoqFZt26dlixZoqysLFVUVEiS3NzcFBERoenTp2vUqFEWd1j/EZoAALiKlJeX64cffpAk+fn5qUmTJhZ31HAQmmA5wzC0efPmKnOPREVFqX///lwMDktVVFTohx9+kM1mU6tWreTm5mZ1SwAswoXgsNTRo0d1yy23aPDgwdqwYYO+/fZbffPNN9qwYYMGDRqk7t276+jRo1a3iQZow4YNioqKUrNmzRQcHKygoCA1a9ZMUVFRF7xIHLhcduzYoTFjxqh9+/by9PRUs2bN1L59e40ZM6bam2lQ+zjSBEuNGDFCxcXFWr16dZU7P3Jzc/XAAw+oRYsWfEnhilq+fLmmTp2qBx98sMrklh9++KFWrlypF198URMmTLC6VTQQTLhaNxCaYKnmzZvrf//3f9W1a9dqx//zn//ojjvuUHFx8RXuDA3ZDTfcoFmzZmn8+PHVjr/22mt65plnmKsJVwwTrtYNnJ6DpTw9PXXy5MkaxwsKCuTp6XkFOwJ+Om18++231zjeq1cvff/991ewIzR0TLhaNxCaYKnRo0dr3Lhxeuutt+RwOJzLHQ6H3nrrLf32t79VbGyshR2iIerUqZP+/ve/1zi+YsUKderU6Qp2hIaOCVfrBia3hKUWLVqks2fPasyYMTp79qzc3d0lSWVlZWrcuLHGjx+vBQsWWNwlGppFixZpyJAhSklJqXZyy++++07vv/++1W2iAWHC1bqBa5pQJxQVFWnnzp06duyYpJ8mEYyIiJC3t7fFnaGhOnjwoJYtW1btpKuTJk1Su3btrG0QDQ4TrlqP0AQAAGACp+dguZKSEq1du7bayS3vv/9+eXl5Wd0iGrDvvvvO5XPZtm1bq1tCA8eEq9bhQnBYau/everYsaNmzpypgoICXXvttWrTpo0KCgr0+9//XqGhodq7d6/VbaIBWrJkiUJCQnTdddcpMjJSPXv21HXXXaeQkBB+GBWWYMJV63F6Dpbq27evAgMDtWrVKudF4JXKysoUHx+v3NxcffzxxxZ1iIboL3/5ixYuXKjZs2dXO7nlvHnzlJiYqMcff9zqVtFAMOFq3UBogqWaNWumnTt3KiwsrNrxnJwc3XbbbTp9+vQV7gwNWUhIiF588UXddddd1Y5v2LBBjz76KD/xgyuGCVfrBk7PwVI+Pj7av39/jePffPONfHx8rmBHgHTixAmFhobWON6xY0cVFBRcwY7Q0DHhat1AaIKlJkyYoHHjxmnhwoX64osvlJeXp2PHjumLL77QwoUL9eCDD+qhhx6yuk00MLfddpueeeYZnT17tsrY2bNnNXfuXN12220WdIaGiglX6wZOz8Fyzz33nF544QXnHUqSZBiGAgMDlZCQoJkzZ1rcIRqa3bt3Kzo6WqWlperdu7fLRIKffvqpPDw8lJaWxpcUrpitW7dqyJAhatu27QUnXL3jjjusbrVeIzShzjhw4IDLhG3t27e3uCM0ZKdOndLq1aurndwyNjaWiVdxxTHhqvUITQAAACYwuSUsd+TIES1btqzK5Ja9evXSpEmTFBISYnWLaKCKi4uVlZXl/FwGBgbqlltuUfPmza1uDQ0YE65ahyNNsFR6eroGDx6skJAQ53n6yrlH0tLSdPjwYX3wwQeKioqyulU0IGfPntWMGTO0YsUK/fjjj3J3d5dhGCovL1fTpk01ceJELViwQE2aNLG6VTQgS5Ys0eLFi/X999+r8qvbZrMpODhYM2bMUEJCgrUNNgAcaYKlpk2bpt/97ndasmRJjeMJCQnasWPHFe4MDdmMGTO0fv16rVy5UjExMWrZsqUkqbCwUB9++KF+//vfSxIzg+OK+aUJV+fMmaPi4mImXL3MONIES3l6eio7O7vGOXG+/PJLdevWTWfOnLnCnaEha926tdatW6d+/fpVO/7RRx9p9OjROn78+BXuDA0VE67WDczTBEsFBQVp27ZtNY5nZGQoKCjoCnYESGfOnJGfn1+N461atSLI44piwtW6gSNNsNRLL72kadOmacKECRo4cGCVuUdeeeUVPf/885o0aZLVraIBGTZsmM6cOaM1a9YoICDAZezYsWOKi4tT06ZN9e6771rUIRqaPn36qE2bNkpKSlLjxq5X1pw9e1bjxo3T0aNH9cknn1jTYANBaILl1q1bpyVLligrK0sVFRWSJDc3N0VERGj69OkaNWqUxR2ioTl8+LDuvPNOffnllwoPD3cJ8zk5OQoLC9OmTZvUpk0bq1tFA8GEq3UDoQl1Rnl5uX744QdJkp+fH3cmwVLnzp3Thx9+WO1EgtHR0WrUiKsbcGUx4ar1CE0AAAAmMOUALLdjxw49//zz1U5uOW3aNHXv3t3qFgEVFBTom2++UVBQEKflYBkmXLUWx5dhqY0bNyoqKkonT57UY489ptdee02vvPKKHnvsMRUUFCgqKkrvvPOO1W2igZk9e7ZOnz4t6afTxhMnTpSfn5969Oihtm3bauTIkfrxxx8t7hINydmzZ/XYY4/J399fffv21bhx4xQXF6c+ffrI399fCQkJKi8vt7rN+s8ALNSpUydj3rx5NY4/++yzRlhY2BXsCDCMRo0aGceOHTMMwzCeeeYZo3Xr1sb69euNo0ePGu+9955xzTXXGE8//bTFXaIhmTp1qnHNNdcYycnJRkFBgXN5QUGBkZycbISEhBiPPfaYZf01FFzTBEs1bdpUu3btUseOHasd/+qrr9S1a1f+rx5XVKNGjZSXlyd/f39169ZNU6ZM0YMPPugcf/PNNzVnzhzt3bvXwi7RkDDhat3A6TlY6vrrr9fGjRtrHH/nnXd03XXXXbmGgP9js9kk/TT9wG233eYydtttt+m7776zoi00UEy4WjdwITgs9fTTT2v06NHaunWr8wd7fz65ZWpqqpKTk61uEw3QihUr1Lx5c3l4eFSZadnhcMjDw8OiztAQ9e3bV9OnT69xwtWZM2fWeBQKtYfQBEvdc889+vTTT/XCCy9o8eLFVeYe2bp1qyIjIy3uEg3NtddeqxUrVkiS3N3d9fnnn+uOO+5wjn/88ccX/EkLoLa99NJLuvPOO9WmTZsLTriKy4trmgDgImVmZsrDw0PdunWzuhU0IEy4aj1CEwBU4/Tp0/rvf/+rzp07Vxnbs2eP2rZty9w4QANDLEWdtm/fPi4EhyXKysrUo0cPffbZZy7L9+7dq27duqm4uNiizoCqSkpK9Omnn1rdRr1HaEKdVlZWxl1KsETLli01bNgwrVq1ymX5G2+8oQEDBigwMNCizoCqvvnmG/Xt29fqNuo9LgSHpaZPn37BceYcgZXGjh2r+Ph4vfDCC2rcuLEMw9CaNWu0cOFCq1sDYAGuaYKl3NzcdPPNN9f469zFxcX6/PPPVVFRcYU7A6SKigq1adNGL7/8skaMGKEtW7bo3nvvVV5entzd3a1uDw2Ir6/vBccrKipUXFzMfysvM440wVIdOnTQtGnT9MADD1Q7np2drYiIiCvcFfATNzc3PfDAA1q1apVGjBihN954Q/fddx+BCVdcaWmpHn744WpvTJCk7777Tk899dQV7qrhITTBUhEREcrKyqoxNNlsNnEwFFYaO3asbrvtNh09elTr169Xamqq1S2hAbr55psVEhKicePGVTv+xRdfEJquAEITLLVo0SKVlpbWON61a1edO3fuCnYEuOrcubPCwsI0ZswYBQcHq2fPnla3hAZoyJAhKiwsrHHc19dXY8eOvXINNVBc0wQAv+Cvf/2rEhIS9Mwzz2jWrFlWtwPAIkw5gDpn8uTJ+uGHH6xuA3B64IEH9OSTT+q3v/2t1a0Akn668HvXrl06e/as1a00KBxpQp3j7e2t7OxsJrUEgBps3LhR99xzj15//XWNGTPG6nYaDI40oc4hxwPAha1atUqtW7dWUlKS1a00KIQmAACuIj/88IM++OADJSUlaevWrTpy5IjVLTUYhCbUOadOneLUHADUYO3atQoPD9egQYN0xx136PXXX7e6pQaDa5pQJxQXFysrK0t5eXmy2WwKCAhQREQEvyIPAOeJiIjQuHHjNHXqVK1cuVLPPfecvvzyS6vbahAITbDU2bNnNWPGDK1YsUI//vij3N3dZRiGysvL1bRpU02cOFELFixQkyZNrG4VACyXk5OjiIgIHT16VH5+fiouLlZAQIC2bNmiHj16WN1evcfpOVhqxowZWr9+vVauXKmTJ0/qxx9/VGlpqU6ePKmVK1fq7bff1u9//3ur2wSAOiEpKUkxMTHy8/OTJDVv3lx33XWXVq5caXFnDQNHmmCp1q1ba926derXr1+14x999JFGjx6t48ePX+HOAKBuqfwB6RdffFH33nuvc/kHH3ygMWPG8EPSVwBHmmCpM2fOOP+PqTqtWrXSmTNnrmBHAFA35efn6+GHH9bw4cNdlsfExGj69OnKy8uzqLOGgyNNsNSwYcN05swZrVmzRgEBAS5jx44dU1xcnJo2bap3333Xog4BAPgJoQmWOnz4sO688059+eWXCg8PV0BAgGw2m/Ly8pSTk6OwsDBt2rRJbdq0sbpVAEADR2iC5c6dO6cPP/xQmZmZzsPLgYGBioyMVHR0tBo14iwyAMB6hCYAAAATGlvdACBJ+/fv17Zt21wmt+zVq5c6dOhgdWsAAEgiNMFiDodDY8eO1XvvvSe73S5/f38ZhqHjx4+rqKhIw4YN0+uvvy5vb2+rWwUANHBcLAJLTZkyRQcOHFBGRoYKCgr01Vdf6euvv1ZBQYG2bdumAwcOaMqUKVa3CQAA1zTBWi1bttSHH35Y4/T/mZmZGjRokAoLC69sYwAAnIcjTbCczWa7pDEAAK4kQhMsNWzYME2YMEE7d+6sMrZz505NmjSpyuy3AABYgdNzsFRhYaHuv/9+ffjhh2rZsqX8/f1ls9l07NgxORwOxcTEaO3atWrZsqXVrQIAGjhCE+qEffv2VTu55Y033mhxZwAA/ITQBAAAYALzNMFyhmFo8+bNVSa3jIqKUv/+/bkYHABQJ3CkCZY6evSohg4dqt27dzt/sNcwDOXn5ysnJ0ddu3bVu+++q2uuucbqVgEADRyhCZYaMWKEiouLtXr1agUFBbmM5ebm6oEHHlCLFi20ceNGaxoEAOD/EJpgqebNm+t///d/1bVr12rH//Of/+iOO+5QcXHxFe4MAABXzNMES3l6eurkyZM1jhcUFMjT0/MKdgQAQPUITbDU6NGjNW7cOL311ltyOBzO5Q6HQ2+99ZZ++9vfKjY21sIOAQD4CXfPwVKLFi3S2bNnNWbMGJ09e1bu7u6SpLKyMjVu3Fjjx4/XggULLO4SAACuaUIdUVRUpJ07d+rYsWOSfprcMiIiQt7e3hZ3BgDATwhNAAAAJnB6DpYrKSnR2rVrq53c8v7775eXl5fVLQIAwJEmWGvv3r0aOHCgTp8+rd69e7tMbrl161Z5eXkpNTVVYWFhVrcKAGjgCE2wVN++fRUYGKhVq1Y5LwKvVFZWpvj4eOXm5urjjz+2qEMAAH5CaIKlmjVrpp07d9Z4JCknJ0e33XabTp8+fYU7AwDAFfM0wVI+Pj7av39/jePffPONfHx8rmBHAABUjwvBYakJEyZo3LhxevzxxzVw4EAFBATIZrMpLy9PaWlpmjt3rhISEqxuEwAATs/Bes8995xeeOEF551zkmQYhgIDA5WQkKCZM2da3CEAAIQm1CEHDhxQXl6epJ8mt2zfvr3FHQEA8P8jNKFOKSgo0KpVq7R//34FBwdr7NixCgkJsbotAAAITbBWcHCwdu/erVatWunAgQOKioqSYRjq3Lmz9u3bp1OnTikzM1M33nij1a0CABo4QhMs1ahRI+Xl5cnf31/333+/8vLytGnTJjVr1kylpaW699571bRpU/3zn/+0ulUAQAPHlAOoM7Zv364nnnhCzZo1kyR5eHjo8ccfV2ZmpsWdAQBAaEIdUHnHXGlpqQICAlzGAgICdPz4cSvaAgDABfM0wXL9+/dX48aNVVRUpK+//lqdOnVyjh06dEh+fn4WdgcAwE8ITbDUk08+6fK88tRcpffee0933HHHlWwJAIBqcSE4AACACVzTBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgDUG/Hx8brrrrusbgNAPUVoAgAAMIHQBKBe6tOnj6ZOnaqZM2fK19dXgYGBmjNnjktNYWGhJk6cqICAADVt2lTh4eH617/+5Rxfv369OnXqJA8PD7Vr106LFi1yeX27du30P//zPxo7dqyaN2+utm3b6p133tHx48c1YsQINW/eXJ07d9bOnTtdXrdt2zb95je/kaenp0JCQjR16lSVlJRctvcCQO0gNAGot1atWiUvLy9t375d8+fP19NPP620tDRJ0rlz5zR48GBt27ZNq1ev1t69e/Xss8/Kzc1NkpSVlaVRo0Zp9OjR2r17t+bMmaMnnnhCSUlJLttYsmSJoqKi9J///EdDhgxRXFycxo4dqwceeECff/65brjhBo0dO1aV8wjv3r1bMTExGjlypHbt2qV169YpPT1djz766BV9bwBcPGYEB1BvxMfHq7CwUBs3blSfPn1UUVGhf//7387x2267Tf369dOzzz6r1NRUDR48WPv27VPHjh2rrGvMmDE6fvy4UlNTnctmzpypTZs2ac+ePZJ+OtJ0xx136I033pAk5eXlKSgoSE888YSefvppSVJmZqYiIyOVm5urwMBAjR07Vp6enlq+fLlzvenp6erdu7dKSkrUtGnTy/LeAPj1ONIEoN7q0qWLy/OgoCDl5+dLkrKzs9WmTZtqA5Mk7du3T1FRUS7LoqKitH//flVUVFS7jYCAAElS586dqyyr3G5WVpaSkpLUvHlz5yMmJkbnzp3TgQMHLnVXAVwB/GAvgHqrSZMmLs9tNpvOnTsnSfL09Lzgaw3DkM1mq7LsQtuorK9uWeV2z507p4ceekhTp06tsq5rr732gj0BsBahCUCD1KVLFx05ckRff/11tUebwsLClJ6e7rJs27Zt6tixo/O6p0txyy23aM+ePbrhhhsueR0ArMHpOQANUu/evfWb3/xG99xzj9LS0nTgwAF98MEHSklJkSTNmDFDH330kf7yl7/o66+/1qpVq7R06VIlJib+qu3+4Q9/UEZGhh555BFlZ2dr//79evfddzVlypTa2C0AlxGhCUCDtX79et166626//77FRYWppkzZzqvV7rlllv05ptvKjk5WeHh4frzn/+sp59+WvHx8b9qm126dNHWrVu1f/9+3XHHHerWrZueeOIJBQUF1cIeAbicuHsOAADABI40AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGDC/wf2Xt6awWH/WgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar plot of the \"Income\" variable\n",
    "df['IncomeEncoded'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Incomoe Levels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and Initial Modeling (2.5 points total)\n",
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: \n",
    "- (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required.\n",
    "\n",
    "- [.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "\n",
    "- [.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  \n",
    "\n",
    "- [.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "\n",
    "- [1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "\n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72718, 40)\n",
      "(72718,)\n",
      "0 72153750602\n",
      "['50000-100000' '<50000' '>100000']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58174, 40)\n",
      "(58174,)\n",
      "(14544, 40)\n",
      "(14544,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "#Original Author: Sebastian Raschka\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "# Start with the following functions:\n",
    "#    init\n",
    "#    encode_labels\n",
    "#    initialize weights\n",
    "#    sigmoid\n",
    "#    add bias (vector of ones)\n",
    "#    objective function (cost and regularizer)\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1 ** 2) + np.mean(W2 ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add in the following functions:\n",
    "#    feedforward\n",
    "#    fit and predict\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # need to vectorize this computation!\n",
    "        # See additional code and derivation below!\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C * 2\n",
    "        gradW2 += W2 * self.l2_C * 2 \n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this is an error fix using co-pilot. idk if thiss is the right thing to do\n",
    "\n",
    "# X_train = X_train.astype(float)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Z1))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(Z1\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Z1' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(Z1))\n",
    "print(Z1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'expit' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:9\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 56\u001b[0m, in \u001b[0;36mTwoLayerPerceptron.fit\u001b[1;34m(self, X, y, print_progress)\u001b[0m\n\u001b[0;32m     53\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# feedforward all instances\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m A1, Z1, A2, Z2, A3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feedforward(X_data,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2)\n\u001b[0;32m     58\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cost(A3,Y_enc,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost_\u001b[38;5;241m.\u001b[39mappend(cost)\n",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m, in \u001b[0;36mTwoLayerPerceptron._feedforward\u001b[1;34m(self, X, W1, W2, b1, b2)\u001b[0m\n\u001b[0;32m     16\u001b[0m A1 \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     17\u001b[0m Z1 \u001b[38;5;241m=\u001b[39m W1 \u001b[38;5;241m@\u001b[39m A1 \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m---> 18\u001b[0m A2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sigmoid(Z1)\n\u001b[0;32m     19\u001b[0m Z2 \u001b[38;5;241m=\u001b[39m W2 \u001b[38;5;241m@\u001b[39m A2 \u001b[38;5;241m+\u001b[39m b2\n\u001b[0;32m     20\u001b[0m A3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sigmoid(Z2)\n",
      "Cell \u001b[1;32mIn[10], line 53\u001b[0m, in \u001b[0;36mTwoLayerPerceptronBase._sigmoid\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use scipy.special.expit to avoid overflow\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 1.0 / (1.0 + np.exp(-z))\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expit(z)\n",
      "File \u001b[1;32md:\\Users\\adeeb\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:2016\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_ufunc__\u001b[39m(\n\u001b[0;32m   2014\u001b[0m     \u001b[38;5;28mself\u001b[39m, ufunc: np\u001b[38;5;241m.\u001b[39mufunc, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39minputs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   2015\u001b[0m ):\n\u001b[1;32m-> 2016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arraylike\u001b[38;5;241m.\u001b[39marray_ufunc(\u001b[38;5;28mself\u001b[39m, ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\adeeb\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:404\u001b[0m, in \u001b[0;36marray_ufunc\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__call__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;66;03m# for np.<ufunc>(..) calls\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# kwargs cannot necessarily be handled block-by-block, so only\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# take this path if there are no kwargs\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[1;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mgetattr\u001b[39m(ufunc, method))\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# otherwise specific ufunc methods (eg np.<ufunc>.accumulate(..))\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# Those can have an axis keyword and thus can't be called block-by-block\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m default_array_ufunc(inputs[\u001b[38;5;241m0\u001b[39m], ufunc, method, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\adeeb\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:350\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m             kwargs[k] \u001b[38;5;241m=\u001b[39m obj[b\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer]\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(f):\n\u001b[1;32m--> 350\u001b[0m     applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\adeeb\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329\u001b[0m, in \u001b[0;36mBlock.apply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'expit' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = dict(n_hidden=50, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=50, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1)\n",
    "\n",
    "nn = TwoLayerPerceptronVectorized(**params)\n",
    "\n",
    "nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is ai generated code, not from class:\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create a 2-layer perceptron with 10 nodes in the first layer and 5 in the second\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = mlp.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6895    0    0]\n",
      " [6104    0    0]\n",
      " [1545    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "50000-100000       0.47      1.00      0.64      6895\n",
      "      <50000       1.00      0.00      0.00      6104\n",
      "     >100000       1.00      0.00      0.00      1545\n",
      "\n",
      "    accuracy                           0.47     14544\n",
      "   macro avg       0.82      0.33      0.21     14544\n",
      "weighted avg       0.75      0.47      0.30     14544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modeling (5 points total)\n",
    "- [1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs.\n",
    "\n",
    "- [1 points] Repeat the previous step, adding support for a fourth layer.\n",
    "\n",
    "- [1 points] Repeat the previous step, adding support for a fifth layer. \n",
    "\n",
    "- [2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (choose either RMSProp or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLinPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
