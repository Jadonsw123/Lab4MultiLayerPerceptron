{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Assignment Four: Multi-Layer Perceptron \n",
    "In this lab, you will compare the performance of multi-layer perceptron programmed  via your own various implementations. \n",
    "\n",
    "This report is worth 10% of the final grade. Please upload a report (one per team) with all code used, visualizations, and text in a rendered Jupyter notebook. Any visualizations that cannot be embedded in the notebook, please provide screenshots of the output. The results should be reproducible using your report. This lab project is slightly different from other reports in that you will be asked to complete more specific items.\n",
    "\n",
    "Dataset Selection\n",
    "\n",
    "For this assignment, you will be using a specific dataset chosen by the instructor.  This is US Census data available on Kaggle, and also downloadable from the following link: https://www.dropbox.com/s/bf7i7qjftk7cmzq/acs2017_census_tract_data.csv?dl=0Links to an external site.\n",
    "\n",
    "The Kaggle description appears here: https://www.kaggle.com/muonneutrino/us-census-demographic-data/dataLinks to an external site. \n",
    "\n",
    "The classification task you will be performing is to predict, for each tract, what the child poverty rate will be. You will need to convert this from regression to four levels of classification by quantizing the variable of interest. \n",
    "\n",
    "Grading Rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Split, and Balance (1.5 points total)\n",
    "[.5 points] \n",
    "- (1) Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric. \n",
    "- (2) Remove any observations that having missing data. \n",
    "- (3) Encode any string data as integers for now. \n",
    "- (4) You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72718, 37)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data into memory and save it to a pandas data frame\n",
    "df = pd.read_csv('./acs2017_census_tract_data.csv')\n",
    "\n",
    "# Remove any observations that have missing data\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode any string data as integers\n",
    "df = df.apply(lambda x: pd.factorize(x)[0] if x.dtype == 'object' else x)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two requirements will need to be completed together as they might depend on one another:\n",
    "- [.5 points] Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. \n",
    "\n",
    "Should balancing of the dataset be done for both the training and testing set? Explain.\n",
    "\n",
    "No, it should only be done for training. Generally, we don't want to touch the testing set because adjusting the distribution of classes in the testing set could introduce bias and affect the evaluation of the model's generalization performance.\n",
    "\n",
    "\n",
    "- [.5 points] Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is no need to split the data multiple times for this lab.\n",
    "Note: You will need to one hot encode the target, but do not one hot encode the categorical data until instructed to do so in the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TractId', 'State', 'County', 'TotalPop', 'Men', 'Women', 'Hispanic',\n",
      "       'White', 'Black', 'Native', 'Asian', 'Pacific', 'VotingAgeCitizen',\n",
      "       'Income', 'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n",
      "       'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n",
      "       'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',\n",
      "       'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n",
      "       'SelfEmployed', 'FamilyWork', 'Unemployment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TractId               int64\n",
      "State                 int64\n",
      "County                int64\n",
      "TotalPop              int64\n",
      "Men                   int64\n",
      "Women                 int64\n",
      "Hispanic            float64\n",
      "White               float64\n",
      "Black               float64\n",
      "Native              float64\n",
      "Asian               float64\n",
      "Pacific             float64\n",
      "VotingAgeCitizen      int64\n",
      "Income              float64\n",
      "IncomeErr           float64\n",
      "IncomePerCap        float64\n",
      "IncomePerCapErr     float64\n",
      "Poverty             float64\n",
      "ChildPoverty        float64\n",
      "Professional        float64\n",
      "Service             float64\n",
      "Office              float64\n",
      "Construction        float64\n",
      "Production          float64\n",
      "Drive               float64\n",
      "Carpool             float64\n",
      "Transit             float64\n",
      "Walk                float64\n",
      "OtherTransp         float64\n",
      "WorkAtHome          float64\n",
      "MeanCommute         float64\n",
      "Employed              int64\n",
      "PrivateWork         float64\n",
      "PublicWork          float64\n",
      "SelfEmployed        float64\n",
      "FamilyWork          float64\n",
      "Unemployment        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TractId             72718\n",
      "State                  52\n",
      "County               1954\n",
      "TotalPop             9771\n",
      "Men                  5639\n",
      "Women                5775\n",
      "Hispanic             1001\n",
      "White                1001\n",
      "Black                1001\n",
      "Native                478\n",
      "Asian                 773\n",
      "Pacific               173\n",
      "VotingAgeCitizen     7352\n",
      "Income              41774\n",
      "IncomeErr           19699\n",
      "IncomePerCap        36838\n",
      "IncomePerCapErr     11093\n",
      "Poverty               796\n",
      "ChildPoverty          954\n",
      "Professional          865\n",
      "Service               579\n",
      "Office                480\n",
      "Construction          483\n",
      "Production            503\n",
      "Drive                 981\n",
      "Carpool               407\n",
      "Transit               831\n",
      "Walk                  556\n",
      "OtherTransp           283\n",
      "WorkAtHome            335\n",
      "MeanCommute           526\n",
      "Employed             5708\n",
      "PrivateWork           673\n",
      "PublicWork            621\n",
      "SelfEmployed          318\n",
      "FamilyWork             73\n",
      "Unemployment          447\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_ranges = [0, 50000, 100000, float('inf')]\n",
    "income_labels = ['<50000', '50000-100000', '>100000']\n",
    "\n",
    "df['IncomeEncoded'] = pd.cut(df['Income'], bins=income_ranges, labels=income_labels)\n",
    "\n",
    "df_encoded = pd.get_dummies(df['IncomeEncoded'])\n",
    "df = pd.concat([df, df_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.dropna()\n",
    "# y = y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TractId               int64\n",
      "State                 int64\n",
      "County                int64\n",
      "TotalPop              int64\n",
      "Men                   int64\n",
      "Women                 int64\n",
      "Hispanic            float64\n",
      "White               float64\n",
      "Black               float64\n",
      "Native              float64\n",
      "Asian               float64\n",
      "Pacific             float64\n",
      "VotingAgeCitizen      int64\n",
      "Income              float64\n",
      "IncomeErr           float64\n",
      "IncomePerCap        float64\n",
      "IncomePerCapErr     float64\n",
      "Poverty             float64\n",
      "ChildPoverty        float64\n",
      "Professional        float64\n",
      "Service             float64\n",
      "Office              float64\n",
      "Construction        float64\n",
      "Production          float64\n",
      "Drive               float64\n",
      "Carpool             float64\n",
      "Transit             float64\n",
      "Walk                float64\n",
      "OtherTransp         float64\n",
      "WorkAtHome          float64\n",
      "MeanCommute         float64\n",
      "Employed              int64\n",
      "PrivateWork         float64\n",
      "PublicWork          float64\n",
      "SelfEmployed        float64\n",
      "FamilyWork          float64\n",
      "Unemployment        float64\n",
      "<50000                 bool\n",
      "50000-100000           bool\n",
      ">100000                bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Iincome' is our target column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df.drop('IncomeEncoded', axis=1).head(10000)  # Features\n",
    "y = df['IncomeEncoded'].head(10000)  # Target\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the target variable and transform it\n",
    "y = le.fit_transform(y)\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Balance the training set by quantizing the \"ChildPoverty\" variable into four classes\n",
    "X_train['ChildPoverty'] = pd.qcut(X_train['ChildPoverty'], q=4, labels=False)\n",
    "print(X.dtypes)\n",
    "\n",
    "# # One-hot encode the target variable\n",
    "# y_train_encoded = pd.get_dummies(y_train)\n",
    "# y_test_encoded = pd.get_dummies(y_test)\n",
    "\n",
    "\n",
    "# # Normalize the continuous numeric feature data\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TractId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IncomeEncoded</th>\n",
       "      <th>&lt;50000</th>\n",
       "      <th>50000-100000</th>\n",
       "      <th>&gt;100000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001020100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1845</td>\n",
       "      <td>899</td>\n",
       "      <td>946</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>881</td>\n",
       "      <td>74.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>50000-100000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001020200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2172</td>\n",
       "      <td>1167</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>852</td>\n",
       "      <td>75.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>&lt;50000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001020300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3385</td>\n",
       "      <td>1533</td>\n",
       "      <td>1852</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1482</td>\n",
       "      <td>73.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>&lt;50000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001020400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4267</td>\n",
       "      <td>2001</td>\n",
       "      <td>2266</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1849</td>\n",
       "      <td>75.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>50000-100000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001020500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9965</td>\n",
       "      <td>5054</td>\n",
       "      <td>4911</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4787</td>\n",
       "      <td>71.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>50000-100000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TractId  State  County  TotalPop   Men  Women  Hispanic  White  Black  \\\n",
       "0  1001020100      0       0      1845   899    946       2.4   86.3    5.2   \n",
       "1  1001020200      0       0      2172  1167   1005       1.1   41.6   54.5   \n",
       "2  1001020300      0       0      3385  1533   1852       8.0   61.4   26.5   \n",
       "3  1001020400      0       0      4267  2001   2266       9.6   80.3    7.1   \n",
       "4  1001020500      0       0      9965  5054   4911       0.9   77.5   16.4   \n",
       "\n",
       "   Native  ...  Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  \\\n",
       "0     0.0  ...       881         74.2        21.2           4.5         0.0   \n",
       "1     0.0  ...       852         75.9        15.0           9.0         0.0   \n",
       "2     0.6  ...      1482         73.3        21.1           4.8         0.7   \n",
       "3     0.5  ...      1849         75.8        19.7           4.5         0.0   \n",
       "4     0.0  ...      4787         71.4        24.1           4.5         0.0   \n",
       "\n",
       "   Unemployment  IncomeEncoded  <50000  50000-100000  >100000  \n",
       "0           4.6   50000-100000   False          True    False  \n",
       "1           3.4         <50000    True         False    False  \n",
       "2           4.7         <50000    True         False    False  \n",
       "3           6.1   50000-100000   False          True    False  \n",
       "4           2.3   50000-100000   False          True    False  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head(5)\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar plot of the \"Income\" variable\n",
    "df['IncomeEncoded'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Income Levels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and Initial Modeling (2.5 points total)\n",
    "You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: \n",
    "- (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). It is recommended to use sigmoids throughout the network, but not required.\n",
    "\n",
    "- [.5 points] Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "\n",
    "- [.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  \n",
    "\n",
    "- [.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. \n",
    "\n",
    "- [1 points] Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances.  \n",
    "\n",
    "Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 40)\n",
      "(10000,)\n",
      "0 6077005130\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 40)\n",
      "(8000,)\n",
      "(2000, 40)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# #Original Author: Sebastian Raschka\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.special import expit\n",
    "# import sys\n",
    "# import pandas as pd\n",
    "\n",
    "# # start with a simple base classifier, which can't be fit or predicted\n",
    "# # it only has internal classes to be used by classes that will subclass it\n",
    "# # Start with the following functions:\n",
    "# #    init\n",
    "# #    encode_labels\n",
    "# #    initialize weights\n",
    "# #    sigmoid\n",
    "# #    add bias (vector of ones)\n",
    "# #    objective function (cost and regularizer)\n",
    "# class TwoLayerPerceptronBase(object):\n",
    "#     def __init__(self, n_hidden=30,\n",
    "#                  C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "#         np.random.seed(random_state)\n",
    "        \n",
    "#         self.n_hidden = n_hidden\n",
    "#         self.l2_C = C\n",
    "#         self.epochs = epochs\n",
    "#         self.eta = eta\n",
    "        \n",
    "#     @staticmethod\n",
    "#     def _encode_labels(y):\n",
    "#         \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "#         onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "#         return onehot\n",
    "\n",
    "#     def _initialize_weights(self):\n",
    "#         \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "#         W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "#         W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "#         W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "#         b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "#         W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "#         W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "#         W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "#         b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "#         return W1, W2, b1, b2\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _sigmoid(z):\n",
    "#         \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "#         # 1.0 / (1.0 + np.exp(-z))\n",
    "#         if isinstance(z, pd.DataFrame) or isinstance(z, pd.Series):\n",
    "#             z = z.values\n",
    "#         z = z.astype(float)\n",
    "#         return expit(z)\n",
    "    \n",
    "    \n",
    "#     @staticmethod\n",
    "#     def _L2_reg(lambda_, W1, W2):\n",
    "#         \"\"\"Compute L2-regularization cost\"\"\"\n",
    "#         # only compute for non-bias terms\n",
    "#         return (lambda_) * np.sqrt(np.mean(W1 ** 2) + np.mean(W2 ** 2))\n",
    "    \n",
    "#     def _cost(self,A3,Y_enc,W1,W2):\n",
    "#         '''Get the objective function value'''\n",
    "#         cost = np.mean((Y_enc-A3)**2)\n",
    "#         L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "#         return cost + L2_term\n",
    "    # Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        if isinstance(z, pd.DataFrame) or isinstance(z, pd.Series):\n",
    "            z = z.values\n",
    "        z = z.astype(float)\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)  # last layer sensitivity\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2) # back prop the sensitivity \n",
    "        \n",
    "        grad2 = V2 @ A2.T # no bias on final layer\n",
    "        grad1 = V1[1:,:] @ A1.T # dont back prop sensitivity of bias\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        \n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add in the following functions:\n",
    "#    feedforward\n",
    "#    fit and predict\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # need to vectorize this computation!\n",
    "        # See additional code and derivation below!\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "#     # just need a different gradient calculation\n",
    "#     def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "#         \"\"\" Compute gradient step using backpropagation.\n",
    "#         \"\"\"\n",
    "#         # vectorized backpropagation\n",
    "#         V2 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "#         V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "#         gradW2 = V2 @ A2.T\n",
    "#         gradW1 = V1 @ A1.T\n",
    "        \n",
    "#         gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "#         gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "#         # regularize weights that are not bias terms\n",
    "#         gradW1 += W1 * self.l2_C * 2\n",
    "#         gradW2 += W2 * self.l2_C * 2 \n",
    "\n",
    "#         return gradW1, gradW2, gradb1, gradb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "# just start with the vectorized version and minibatch\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        rho_W1_prev = np.zeros(self.W1.shape)\n",
    "        rho_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # \\frac{\\eta}{1+\\epsilon\\cdot k}\n",
    "            eta = self.eta / (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "            X_data = X_data.reset_index(drop=True)\n",
    "            # Now you can shuffle\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data.iloc[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "            # if self.shuffle:\n",
    "            #     idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "            #     X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data.iloc[idx], self.W1, self.W2)\n",
    "                # A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                #                                        self.W1,\n",
    "                #                                        self.W2)\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                print(type(eta),type(grad1))\n",
    "                print(grad1.dtype) #object\n",
    "                rho_W1, rho_W2 = eta * grad1, eta * grad2\n",
    "                print(type(rho_W1), type(self.alpha), type(rho_W1_prev))\n",
    "                print(rho_W1.dtype,rho_W1_prev.dtype)\n",
    "                # print(np.isnan(rho_W1).any())\n",
    "                # print(np.isnan(rho_W1_prev).any())\n",
    "                print(\"self.w1=\",type(self.W1))\n",
    "                \n",
    "                temp=rho_W1 + (self.alpha * rho_W1_prev)\n",
    "                # Assuming `temp` is the NumPy array\n",
    "                # # random_indices = np.random.choice(temp.size, size=5, replace=False)\n",
    "                # # random_elements = temp[random_indices]\n",
    "                # # data_types = random_elements.dtype\n",
    "\n",
    "                # print(data_types)\n",
    "                print(temp.dtype)\n",
    "                self.W1 -= rho_W1 + (self.alpha * rho_W1_prev)\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev))\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement the new style of objective function, \n",
    "# we just need to update the final layer calculation of the gradient\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "        print(\"hello\")\n",
    "        return grad1, grad2\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "        \n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden + 1)) \n",
    "        W2[:,:1] = 0\n",
    "        \n",
    "        return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this is an error fix using co-pilot. idk if thiss is the right thing to do\n",
    "\n",
    "# X_train = X_train.astype(float)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(Z1))\n",
    "# print(Z1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'> <class 'numpy.ndarray'>\n",
      "object\n",
      "<class 'numpy.ndarray'> <class 'float'> <class 'numpy.ndarray'>\n",
      "object float64\n",
      "self.w1= <class 'numpy.ndarray'>\n",
      "object\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'subtract' output from dtype('O') to dtype('float64') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[417], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.001\u001b[39m\n\u001b[0;32m     15\u001b[0m nn_better \u001b[38;5;241m=\u001b[39m TLPBetterInitial(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 17\u001b[0m nn_better\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, print_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, XY_test\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# nn = TLPBetterInitial(**params)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# nn.fit(X_train, y_train, print_progress=50)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m yhat \u001b[38;5;241m=\u001b[39m nn_better\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[412], line 94\u001b[0m, in \u001b[0;36mTLPMiniBatch.fit\u001b[1;34m(self, X, y, print_progress, XY_test)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Assuming `temp` is the NumPy array\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# # random_indices = np.random.choice(temp.size, size=5, replace=False)\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# # random_elements = temp[random_indices]\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# # data_types = random_elements.dtype\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# print(data_types)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(temp\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m rho_W1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m rho_W1_prev)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (rho_W2 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m rho_W2_prev))\n\u001b[0;32m     96\u001b[0m rho_W1_prev, rho_W2_prev \u001b[38;5;241m=\u001b[39m rho_W1, rho_W2\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'subtract' output from dtype('O') to dtype('float64') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# params = dict(n_hidden=50, \n",
    "#               C=0.1, # tradeoff L2 regularizer\n",
    "#               epochs=50, # iterations\n",
    "#               eta=0.001,  # learning rate\n",
    "#               random_state=1)\n",
    "params = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':20, 'eta':0.001, \n",
    "         'alpha':0.001, 'decrease_const':1e-5, 'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "params['epochs'] = 10\n",
    "params['eta'] = .001\n",
    "nn_better = TLPBetterInitial(**params)\n",
    "\n",
    "nn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "# nn = TLPBetterInitial(**params)\n",
    "\n",
    "# nn.fit(X_train, y_train, print_progress=50)\n",
    "yhat = nn_better.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is ai generated code, not from class:\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create a 2-layer perceptron with 10 nodes in the first layer and 5 in the second\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = mlp.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[920   0   0]\n",
      " [813   0   0]\n",
      " [267   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      1.00      0.63       920\n",
      "           1       1.00      0.00      0.00       813\n",
      "           2       1.00      0.00      0.00       267\n",
      "\n",
      "    accuracy                           0.46      2000\n",
      "   macro avg       0.82      0.33      0.21      2000\n",
      "weighted avg       0.75      0.46      0.29      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [.5 points] Now (1) normalize the continuous numeric feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "c:\\Users\\jadon\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "c:\\Users\\jadon\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "c:\\Users\\jadon\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "Epoch: 50/50C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
      "C:\\Users\\jadon\\AppData\\Local\\Temp\\ipykernel_7772\\1095048132.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.913\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on the training set and transform both the training and test set\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "params = dict(n_hidden=50, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=50, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1)\n",
    "\n",
    "nn = TLPBetterInitial(**params)\n",
    "\n",
    "nn.fit(X_train_scaled, y_train, print_progress=50)\n",
    "yhat = nn.predict(X_test_scaled)\n",
    "print('Accuracy:',accuracy_score(y_test,yhat))\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[.5 points] Now(1) normalize the continuous numeric feature data AND (2) one hot encode the categorical data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([339, 677,  39,  93,  14, 604, 130, 577, 683, 219,\\n       ...\\n       357,  58, 470, 779, 377, 191, 262, 449,  82, 761],\\n      dtype='int32', length=800)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 48\u001b[0m\n\u001b[0;32m     40\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(n_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \n\u001b[0;32m     41\u001b[0m               C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;66;03m# tradeoff L2 regularizer\u001b[39;00m\n\u001b[0;32m     42\u001b[0m               epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \u001b[38;5;66;03m# iterations\u001b[39;00m\n\u001b[0;32m     43\u001b[0m               eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,  \u001b[38;5;66;03m# learning rate\u001b[39;00m\n\u001b[0;32m     44\u001b[0m               random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m nn \u001b[38;5;241m=\u001b[39m TLPBetterInitial(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 48\u001b[0m nn\u001b[38;5;241m.\u001b[39mfit(X_train_preprocessed, y_train_preprocessed, print_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     49\u001b[0m yhat_preprocessed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mpredict(X_test_preprocessed)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m,accuracy_score(y_test_preprocessed,yhat_preprocessed))\n",
      "Cell \u001b[1;32mIn[40], line 51\u001b[0m, in \u001b[0;36mTLPMiniBatch.fit\u001b[1;34m(self, X, y, print_progress, XY_test)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle:\n\u001b[0;32m     50\u001b[0m     idx_shuffle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(y_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 51\u001b[0m     X_data, Y_enc, y_data \u001b[38;5;241m=\u001b[39m X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n\u001b[0;32m     53\u001b[0m mini \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(\u001b[38;5;28mrange\u001b[39m(y_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminibatches)\n\u001b[0;32m     54\u001b[0m mini_cost \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\jadon\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jadon\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jadon\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([339, 677,  39,  93,  14, 604, 130, 577, 683, 219,\\n       ...\\n       357,  58, 470, 779, 377, 191, 262, 449,  82, 761],\\n      dtype='int32', length=800)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "df_preprocessed = pd.read_csv('./acs2017_census_tract_data.csv')\n",
    "\n",
    "# Remove any observations that have missing data\n",
    "df_preprocessed.dropna(inplace=True)\n",
    "df_preprocessed = df_preprocessed.drop('County',axis = 1)\n",
    "\n",
    "df_preprocessed_encoded = pd.get_dummies(df_preprocessed['State'], prefix='State')\n",
    "df_preprocessed = pd.concat([df_preprocessed, df_preprocessed_encoded], axis=1)\n",
    "df_preprocessed = df_preprocessed.drop('State', axis=1)\n",
    "# print(df_preprocessed.dtypes)\n",
    "# print(df_preprocessed.nunique())\n",
    "income_ranges = [0, 50000, 100000, float('inf')]\n",
    "income_labels = ['<50000', '50000-100000', '>100000']\n",
    "\n",
    "df_preprocessed['IncomeEncoded'] = pd.cut(df_preprocessed['Income'], bins=income_ranges, labels=income_labels)\n",
    "\n",
    "df_preprocessed_encoded = pd.get_dummies(df_preprocessed['IncomeEncoded'])\n",
    "df_preprocessed = pd.concat([df_preprocessed, df_preprocessed_encoded], axis=1)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_preprocessed = df_preprocessed.drop('IncomeEncoded', axis=1).head(1000)  # Features\n",
    "y_preprocessed = df_preprocessed['IncomeEncoded'].head(1000)  # Target\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the target variable and transform it\n",
    "y_preprocessed = le.fit_transform(y_preprocessed)\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train_preprocessed, X_test_preprocessed, y_train_preprocessed, y_test_preprocessed = train_test_split(X_preprocessed, y_preprocessed, test_size=0.2, random_state=42, stratify=y_preprocessed)\n",
    "\n",
    "# Balance the training set by quantizing the \"ChildPoverty\" variable into four classes\n",
    "X_train_preprocessed['ChildPoverty'] = pd.qcut(X_train_preprocessed['ChildPoverty'], q=4, labels=False)\n",
    "\n",
    "\n",
    "params = dict(n_hidden=50, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=50, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1)\n",
    "\n",
    "nn = TLPBetterInitial(**params)\n",
    "\n",
    "nn.fit(X_train_preprocessed, y_train_preprocessed, print_progress=50)\n",
    "yhat_preprocessed = nn.predict(X_test_preprocessed)\n",
    "print('Accuracy:',accuracy_score(y_test_preprocessed,yhat_preprocessed))\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modeling (5 points total)\n",
    "- [1 points] Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
    "\n",
    "Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs.\n",
    "\n",
    "- [1 points] Repeat the previous step, adding support for a fourth layer.\n",
    "\n",
    "- [1 points] Repeat the previous step, adding support for a fifth layer. \n",
    "\n",
    "- [2 points] Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (choose either RMSProp or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. Do not use AdaM for the adaptive learning technique as it is part of the exceptional work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLinPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
